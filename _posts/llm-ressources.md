---
title: 'Some of the resources I have gathered on Large Language Models (LLMs)'
date: 2025-04-01
permalink: /posts/2025/04/llm-resources/
tags:
  - LLMs
---

# ğŸ“š Learning Resources on Large Language Models (LLMs)


### â–¶ï¸ **General LLM Intuition & Architectures**

* **[Andrej Karpathy â€“ LLM Series](https://www.youtube.com/@AndrejKarpathy)**

  * Must-watch series covering GPT-1, GPT-2, tokenizers, and LLM internals.
* **[Rotary Positional Embeddings](https://www.youtube.com/watch?v=o29P0Kpobz0)**
*  **[Yann Dubois â€“ Stanford CS229 I Building Large Language Models (LLMs)](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=1021s)**
---

## ğŸ” **Key Topics in Depth**

### ğŸ§® ***RLHF***

* **[Julia Turc](https://www.youtube.com/@juliaturc1)**

  * ğŸ”— [Proximal Policy Optimization (PPO) for LLMs Explained Intuitively
](https://www.youtube.com/watch?v=8jtAzxUwDj0)
  * ğŸ”— [DeepSeek's GRPO (Group Relative Policy Optimization) | Reinforcement Learning for LLMs
](https://www.youtube.com/watch?v=xT4jxQUl0X8&t=990s)

### ğŸ§® ***Efficient inference***

* **[Efficient NLP](https://www.youtube.com/@EfficientNLP)**

  * ğŸ”— [Speculative Decoding](https://www.youtube.com/watch?v=S-8yr_RibJ4)
  * ğŸ”— [KV-Cache](https://www.youtube.com/watch?v=80bIUggRJf4)

### ğŸ§® **Quantization & Compression**

* ğŸ”— [Quantization 101](https://www.youtube.com/watch?v=0VdNflU08yA)

  * Great starting point for understanding quantization in LLMs.
* ğŸ”— [QLoRA Explained](https://www.youtube.com/watch?v=XpoKB3usmKc)

  * Efficient fine-tuning technique for large models.

### ğŸ§  **Mixture of Experts (MoE)**

* ğŸ“° [Nano-MoE by Cameron Wolfe](https://cameronrwolfe.substack.com/p/nano-moe)

  * Walks through the step-by-step construction of each component of the nanoMoE model. 

### ğŸ“Š **RAG & Vector Databases**

* ğŸ”— [How Vector Databases Work (RAG)](https://www.youtube.com/watch?v=035I2WKj5F0)


### ğŸ“· **Multimodal Learning**

* ğŸ”— [Mixed-modal Language Modeling: Chameleon, Transfusion, and Mixture of Transformers](https://www.youtube.com/watch?v=JYMXlmSM_Ew&t=3494s)

  * Covers a series of early fusion mixed-modal models trained on arbitrary mixed sequences of images and text.

---

## ğŸ§  **Courses**

* **[Hugging Face Agents Course](https://huggingface.co/learn/agents-course/unit0/introduction)**

  * Beginner-friendly course introducing agent-based architectures using LLMs.

---

## ğŸ”¬ **Research Papers**

[The latest LLM research shows how they are getting SMARTER and FASTER.](https://www.youtube.com/watch?v=_Y3BfN9v3sA&t=2s)
### ğŸ”§ **Model Optimization & Compression**

* **[MiniLLM: Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2306.08543)**


* **[GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers](https://arxiv.org/abs/2210.17323)**

* **[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)**


* **[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)**

* **[MEGABLOCKS: Efficient Sparse Training with Mixture-of-Experts](https://arxiv.org/abs/2211.15841)**


* **[Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation](https://arxiv.org/abs/2410.02725)**


### ğŸ§  **Alignment & Instruction Tuning**

* **[Instruction-Following Evaluation for Large Language Models](https://arxiv.org/abs/2311.07911)**


* **[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)**


* **[SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/abs/2212.10560)**


* **[Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)**



### ğŸ“š **Training & Architecture**

* **[GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245)**

* **[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)**

* **[TRANSFORMER-SQUARED: Self-Adaptive LLMs](https://arxiv.org/abs/2501.06252)**

* **[Why Do We Need Weight Decay in Modern Deep Learning?](https://arxiv.org/abs/2310.04415)**

* **[DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/abs/2401.06066)**

* **[Scalable-Softmax Is Superior for Attention](https://arxiv.org/abs/2501.19399)**


* **[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)**


* **[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)**


* **[The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)**


* **[Finetuned Language Models are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)**


### ğŸ§© **RAG & External Knowledge**

* **[Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)**


* **[Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models](https://arxiv.org/abs/2311.06233)**


### ğŸŒ **Multimodal & Prompt Techniques**

* **[Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/abs/2405.09818)**

  * Introduces a model capable of processing and generating mixed-modal data.

* **[P-ADAPTERS: Robustly Extracting Factual Information from Language Models with Diverse Prompts](https://arxiv.org/abs/2110.07280)**

  * Proposes a method for robust factual extraction using diverse prompts.

---

## ğŸ’¼ **Content I found funny**

* ğŸ”— [How programmers overprepare for job interviews](https://www.youtube.com/watch?v=5bId3N7QZec)

---

Let me know if you'd like a downloadable `.md` version or a visual mind map! Would you like me to clean up the duplicate entries or tag the papers by difficulty level too?
