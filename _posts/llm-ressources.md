---
title: 'Some of the resources I have gathered on Large Language Models (LLMs)'
date: 2025-04-01
permalink: /posts/2025/04/llm-resources/
tags:
  - LLMs
---

# 📚 Learning Resources on Large Language Models (LLMs)


### ▶️ **General LLM Intuition & Architectures**

* **[Andrej Karpathy – LLM Series](https://www.youtube.com/@AndrejKarpathy)**

  * Must-watch series covering GPT-1, GPT-2, tokenizers, and LLM internals.
* **[Rotary Positional Embeddings](https://www.youtube.com/watch?v=o29P0Kpobz0)**
*  **[Yann Dubois – Stanford CS229 I Building Large Language Models (LLMs)](https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=1021s)**
---

## 🔍 **Key Topics in Depth**

### 🧮 ***RLHF***

* **[Julia Turc](https://www.youtube.com/@juliaturc1)**

  * 🔗 [Proximal Policy Optimization (PPO) for LLMs Explained Intuitively
](https://www.youtube.com/watch?v=8jtAzxUwDj0)
  * 🔗 [DeepSeek's GRPO (Group Relative Policy Optimization) | Reinforcement Learning for LLMs
](https://www.youtube.com/watch?v=xT4jxQUl0X8&t=990s)

### 🧮 ***Efficient inference***

* **[Efficient NLP](https://www.youtube.com/@EfficientNLP)**

  * 🔗 [Speculative Decoding](https://www.youtube.com/watch?v=S-8yr_RibJ4)
  * 🔗 [KV-Cache](https://www.youtube.com/watch?v=80bIUggRJf4)

### 🧮 **Quantization & Compression**

* 🔗 [Quantization 101](https://www.youtube.com/watch?v=0VdNflU08yA)

  * Great starting point for understanding quantization in LLMs.
* 🔗 [QLoRA Explained](https://www.youtube.com/watch?v=XpoKB3usmKc)

  * Efficient fine-tuning technique for large models.

### 🧠 **Mixture of Experts (MoE)**

* 📰 [Nano-MoE by Cameron Wolfe](https://cameronrwolfe.substack.com/p/nano-moe)

  * Walks through the step-by-step construction of each component of the nanoMoE model. 

### 📊 **RAG & Vector Databases**

* 🔗 [How Vector Databases Work (RAG)](https://www.youtube.com/watch?v=035I2WKj5F0)


### 📷 **Multimodal Learning**

* 🔗 [Mixed-modal Language Modeling: Chameleon, Transfusion, and Mixture of Transformers](https://www.youtube.com/watch?v=JYMXlmSM_Ew&t=3494s)

  * Covers a series of early fusion mixed-modal models trained on arbitrary mixed sequences of images and text.

---

## 🧠 **Courses**

* **[Hugging Face Agents Course](https://huggingface.co/learn/agents-course/unit0/introduction)**

  * Beginner-friendly course introducing agent-based architectures using LLMs.

---

## 🔬 **Research Papers**

[The latest LLM research shows how they are getting SMARTER and FASTER.](https://www.youtube.com/watch?v=_Y3BfN9v3sA&t=2s)
### 🔧 **Model Optimization & Compression**

* **[MiniLLM: Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2306.08543)**


* **[GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers](https://arxiv.org/abs/2210.17323)**

* **[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)**


* **[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)**

* **[MEGABLOCKS: Efficient Sparse Training with Mixture-of-Experts](https://arxiv.org/abs/2211.15841)**


* **[Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation](https://arxiv.org/abs/2410.02725)**


### 🧠 **Alignment & Instruction Tuning**

* **[Instruction-Following Evaluation for Large Language Models](https://arxiv.org/abs/2311.07911)**


* **[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)**


* **[SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/abs/2212.10560)**


* **[Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)**



### 📚 **Training & Architecture**

* **[GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245)**

* **[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)**

* **[TRANSFORMER-SQUARED: Self-Adaptive LLMs](https://arxiv.org/abs/2501.06252)**

* **[Why Do We Need Weight Decay in Modern Deep Learning?](https://arxiv.org/abs/2310.04415)**

* **[DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/abs/2401.06066)**

* **[Scalable-Softmax Is Superior for Attention](https://arxiv.org/abs/2501.19399)**


* **[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)**


* **[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)**


* **[The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)**


* **[Finetuned Language Models are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)**


### 🧩 **RAG & External Knowledge**

* **[Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)**


* **[Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models](https://arxiv.org/abs/2311.06233)**


### 🌐 **Multimodal & Prompt Techniques**

* **[Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/abs/2405.09818)**

  * Introduces a model capable of processing and generating mixed-modal data.

* **[P-ADAPTERS: Robustly Extracting Factual Information from Language Models with Diverse Prompts](https://arxiv.org/abs/2110.07280)**

  * Proposes a method for robust factual extraction using diverse prompts.

---

## 💼 **Content I found funny**

* 🔗 [How programmers overprepare for job interviews](https://www.youtube.com/watch?v=5bId3N7QZec)

---

Let me know if you'd like a downloadable `.md` version or a visual mind map! Would you like me to clean up the duplicate entries or tag the papers by difficulty level too?
